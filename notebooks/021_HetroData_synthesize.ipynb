{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp HeteroData_synthesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 1;\n                var nbb_unformatted_code = \"%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n%matplotlib inline\";\n                var nbb_formatted_code = \"%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n%matplotlib inline\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": "<IPython.core.display.Javascript object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 2;\n                var nbb_unformatted_code = \"# export\\nimport numpy as np\\nfrom datetime import datetime\\nfrom sklearn.utils import shuffle\\nfrom sklearn.model_selection import train_test_split\\nfrom typing import Tuple, Optional, Dict\\nimport pytorch_lightning as pl\\nimport torch\\nfrom torch_geometric.utils import to_dense_batch\\nfrom torch_geometric.data import HeteroData\\nimport torch_geometric.nn as pygnn\\nfrom torch_geometric.loader import DataLoader\\nfrom pathlib import Path\";\n                var nbb_formatted_code = \"# export\\nimport numpy as np\\nfrom datetime import datetime\\nfrom sklearn.utils import shuffle\\nfrom sklearn.model_selection import train_test_split\\nfrom typing import Tuple, Optional, Dict\\nimport pytorch_lightning as pl\\nimport torch\\nfrom torch_geometric.utils import to_dense_batch\\nfrom torch_geometric.data import HeteroData\\nimport torch_geometric.nn as pygnn\\nfrom torch_geometric.loader import DataLoader\\nfrom pathlib import Path\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": "<IPython.core.display.Javascript object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Tuple, Optional, Dict\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.nn as pygnn\n",
    "from torch_geometric.loader import DataLoader\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 3;\n                var nbb_unformatted_code = \"#export\\ndef synthesize_mixing_vector(length, random_state=46, **kwargs):\\n    \\\"\\\"\\\"\\n\\n    Args:\\n        length (int) : length = number_of_nodes*number of features\\n        random_state (int): seed for random shuffling\\n\\n    Returns:\\n        np.ndarray mixing vector\\n    \\\"\\\"\\\"\\n    indices = np.arange(length)\\n    return shuffle(indices, random_state=random_state)\\n\\n\\ndef generate_positive_negative_data(length, total_samples=10000, **kwargs):\\n    \\\"\\\"\\\"\\n\\n    Args:\\n        length (int) : length = number_of_nodes*number of features\\n        total_samples (int): total number of instances\\n        **kwargs:\\n\\n    Returns:\\n        Tuple[np.ndarray,np.ndarray] for positive and negative data\\n    \\\"\\\"\\\"\\n    mixing_vec_pos = synthesize_mixing_vector(length, **kwargs)\\n    A_pos = np.random.randn(length, length)\\n    cov_pos = np.matmul(A_pos.T, A_pos)\\n    mean_pos = np.amax(np.abs(cov_pos.flatten())) * (1 + np.random.randn(length) * 0.25)\\n    raw_samples_pos = np.random.multivariate_normal(mean_pos, cov_pos, size=total_samples)\\n    samples_pos = raw_samples_pos[:, mixing_vec_pos]\\n\\n    mixing_vec_neg = synthesize_mixing_vector(length, **kwargs)\\n    A_neg = np.random.randn(length, length)\\n    cov_neg = np.matmul(A_neg.T, A_neg)\\n    mean_neg = np.amax(np.abs(cov_neg.flatten())) * (-1 + np.random.randn(length) * 0.25)\\n    raw_samples_neg = np.random.multivariate_normal(mean_neg, cov_neg, size=total_samples)\\n    samples_neg = raw_samples_neg[:, mixing_vec_neg]\\n    return samples_pos, samples_neg\\n\\n\\n# noinspection PyPep8Naming\\ndef get_data_list(number_of_nodes=15, number_of_features=5, total_samples=10000, **kwargs):\\n    X_p, X_n = generate_positive_negative_data(number_of_nodes * number_of_features, total_samples)\\n    Y = [0] * X_n.shape[0] + [1] * X_p.shape[0]\\n    X_p = np.reshape(X_p, (total_samples, number_of_nodes, number_of_features))\\n    X_n = np.reshape(X_n, (total_samples, number_of_nodes, number_of_features))\\n    X = np.concatenate([X_n, X_p], axis=0)\\n    edge_index_dict = {\\\"type1\\\": [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\\n                                 [0, 0, 0, 0, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4]],\\n                       \\\"type2\\\": [[0, 0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14],\\n                                 [1, 5, 7, 7, 7, 9, 6, 7, 9, 14, 11, 6, 7, 7, 13]]}\\n    data_list = [HeteroData(\\n        {\\\"y\\\": torch.tensor(y, dtype=torch.long),\\n         \\\"node_a\\\": {\\\"x\\\": torch.tensor(x, dtype=torch.float32)},\\n         (\\\"node_a\\\", \\\"love\\\", \\\"node_a\\\"): {\\\"edge_index\\\": torch.tensor(edge_index_dict[\\\"type1\\\"], dtype=torch.long)},\\n         (\\\"node_a\\\", \\\"hate\\\", \\\"node_a\\\"): {\\\"edge_index\\\": torch.tensor(edge_index_dict[\\\"type2\\\"], dtype=torch.long)}\\n         })\\n        for x, y in zip(X, Y)]\\n    return data_list\\n\\n\\nclass SyntheticDataModule(pl.LightningDataModule):\\n    def __init__(self, batch_size=100, **kwargs):\\n        super().__init__()\\n        self.batch_size = batch_size\\n\\n        full_data_list = get_data_list(**kwargs)\\n        self.train_data_list, self.test_data_list = train_test_split(full_data_list, test_size=0.2, shuffle=True)\\n        self.static_edge_index_dict = full_data_list[0].edge_index_dict\\n\\n    def train_dataloader(self):\\n        return DataLoader(self.train_data_list, batch_size=self.batch_size)\\n\\n    def test_dataloader(self):\\n        return DataLoader(self.test_data_list, batch_size=self.batch_size)\\n\\n    def get_single_sample(self):\\n        return self.train_data_list[0]\\n\\n\\nclass HeteroGNNArch(torch.nn.Module):\\n    def __init__(self, hidden_channels, out_channels, num_layers, number_of_labels=2):\\n        super().__init__()\\n\\n        self.convs = torch.nn.ModuleList()\\n        for _ in range(num_layers):\\n            conv = pygnn.HeteroConv({\\n                (\\\"node_a\\\", \\\"love\\\", \\\"node_a\\\"): pygnn.GCNConv(-1, hidden_channels),\\n                (\\\"node_a\\\", \\\"hate\\\", \\\"node_a\\\"): pygnn.SAGEConv((-1, -1), hidden_channels),\\n            }, aggr='sum')\\n            self.convs.append(conv)\\n\\n        self.lin = pygnn.Linear(-1, hidden_channels)\\n\\n        self.dense = torch.nn.LazyLinear(number_of_labels)\\n        self.softmax = torch.nn.Softmax(dim=0)\\n\\n    def forward(self, x_dict, edge_index_dict, batch_dict: Optional[Dict] = None):\\n        # todo: make sure x_dict.keys() is orderd like batch_vector\\n        for conv in self.convs:\\n            x_dict = conv(x_dict, edge_index_dict)\\n            x_dict = {key: x.relu() for key, x in x_dict.items()}\\n        x_dict = {key: self.lin(xx) for key, xx in x_dict.items()}\\n        x_dict_batched = {key: torch.flatten(to_dense_batch(xx, b_vec)[0], start_dim=1) for (key, xx), b_vec in\\n                          zip(x_dict.items(), batch_dict.values())}\\n        x = torch.cat(list(x_dict_batched.values()))\\n        x = self.dense(x)\\n        output = self.softmax(x)\\n        # TODO:  try a layer that recive a dictonery(diffrent kind of nodes)_\\n        return output\\n\\n\\nclass RamSimpleModel(pl.LightningModule):\\n    def __init__(self, model_arch: torch.nn.Module, **kwargs):\\n        super().__init__()\\n        self.model_arch = model_arch\\n\\n        self.static_edge_index_dict = None\\n\\n    def training_step(self, batch, batch_idx):\\n        x_dict, y_t, edge_index_dict, batch_dict = batch.x_dict, batch.y, batch.edge_index_dict, batch.batch_dict\\n        # edge_index_dict = {key:torch.tile(x,[y.size()[0],1,1]) for key,x in self.static_edge_index_dict.items()}\\n        pred = self.model_arch(x_dict, edge_index_dict, batch_dict)\\n        y_t_onehot = torch.tensor(torch.nn.functional.one_hot(y_t, 2), dtype=torch.float32)\\n        train_loss = torch.nn.functional.binary_cross_entropy(pred, y_t_onehot)\\n        correct = pred.argmax(dim=1).eq(y_t).sum().item()\\n        total = pred.size()[0]\\n\\n        logs = {\\\"train_loss\\\": train_loss}\\n        batch_logs_dictionary = {\\n            \\\"loss\\\": train_loss,\\n            \\\"log\\\": logs,\\n            \\\"correct\\\": correct,\\n            \\\"total\\\": total,\\n        }\\n        return batch_logs_dictionary\\n\\n    def training_epoch_end(self, outputs) -> None:\\n        avg_loss = torch.stack([xx['loss'] for xx in outputs]).mean()\\n        correct = sum([x[\\\"correct\\\"] for x in outputs])\\n        total = sum([x[\\\"total\\\"] for x in outputs])\\n\\n        self.logger.experiment.add_scalar(\\\"Loss/Train\\\", avg_loss, self.current_epoch)\\n        self.logger.experiment.add_scalar(\\\"Accuracy/Train\\\", correct / total, self.current_epoch)\\n\\n    def configure_optimizers(self):\\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\\n        return optimizer\\n\\n    def lazy_initialize(self, sample):\\n        # x_dict = {key:torch.tile(x,[batch_size,1,1]) for key,x in sample.x_dict.items()}\\n        # edge_index_dict = {key:torch.tile(x,[batch_size,1,1]) for key,x in sample.edge_index_dict.items()}\\n        batch_dict = {key: torch.zeros(xx.size()[0], dtype=torch.long) for key, xx in sample.x_dict.items()}\\n        with torch.no_grad():  # Initialize lazy modules.\\n            _ = self.model_arch(sample.x_dict, sample.edge_index_dict, batch_dict=batch_dict)\\n        self.logger.experiment.add_graph(RamSimpleModel(self.model_arch),(sample.x_dict, sample.edge_index_dict,batch_dict))\\n\\n    def set_static_edge_index(self, edge_index):\\n        self.static_edge_index_dict = edge_index\";\n                var nbb_formatted_code = \"# export\\ndef synthesize_mixing_vector(length, random_state=46, **kwargs):\\n    \\\"\\\"\\\"\\n\\n    Args:\\n        length (int) : length = number_of_nodes*number of features\\n        random_state (int): seed for random shuffling\\n\\n    Returns:\\n        np.ndarray mixing vector\\n    \\\"\\\"\\\"\\n    indices = np.arange(length)\\n    return shuffle(indices, random_state=random_state)\\n\\n\\ndef generate_positive_negative_data(length, total_samples=10000, **kwargs):\\n    \\\"\\\"\\\"\\n\\n    Args:\\n        length (int) : length = number_of_nodes*number of features\\n        total_samples (int): total number of instances\\n        **kwargs:\\n\\n    Returns:\\n        Tuple[np.ndarray,np.ndarray] for positive and negative data\\n    \\\"\\\"\\\"\\n    mixing_vec_pos = synthesize_mixing_vector(length, **kwargs)\\n    A_pos = np.random.randn(length, length)\\n    cov_pos = np.matmul(A_pos.T, A_pos)\\n    mean_pos = np.amax(np.abs(cov_pos.flatten())) * (1 + np.random.randn(length) * 0.25)\\n    raw_samples_pos = np.random.multivariate_normal(\\n        mean_pos, cov_pos, size=total_samples\\n    )\\n    samples_pos = raw_samples_pos[:, mixing_vec_pos]\\n\\n    mixing_vec_neg = synthesize_mixing_vector(length, **kwargs)\\n    A_neg = np.random.randn(length, length)\\n    cov_neg = np.matmul(A_neg.T, A_neg)\\n    mean_neg = np.amax(np.abs(cov_neg.flatten())) * (\\n        -1 + np.random.randn(length) * 0.25\\n    )\\n    raw_samples_neg = np.random.multivariate_normal(\\n        mean_neg, cov_neg, size=total_samples\\n    )\\n    samples_neg = raw_samples_neg[:, mixing_vec_neg]\\n    return samples_pos, samples_neg\\n\\n\\n# noinspection PyPep8Naming\\ndef get_data_list(\\n    number_of_nodes=15, number_of_features=5, total_samples=10000, **kwargs\\n):\\n    X_p, X_n = generate_positive_negative_data(\\n        number_of_nodes * number_of_features, total_samples\\n    )\\n    Y = [0] * X_n.shape[0] + [1] * X_p.shape[0]\\n    X_p = np.reshape(X_p, (total_samples, number_of_nodes, number_of_features))\\n    X_n = np.reshape(X_n, (total_samples, number_of_nodes, number_of_features))\\n    X = np.concatenate([X_n, X_p], axis=0)\\n    edge_index_dict = {\\n        \\\"type1\\\": [\\n            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\\n            [0, 0, 0, 0, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4],\\n        ],\\n        \\\"type2\\\": [\\n            [0, 0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14],\\n            [1, 5, 7, 7, 7, 9, 6, 7, 9, 14, 11, 6, 7, 7, 13],\\n        ],\\n    }\\n    data_list = [\\n        HeteroData(\\n            {\\n                \\\"y\\\": torch.tensor(y, dtype=torch.long),\\n                \\\"node_a\\\": {\\\"x\\\": torch.tensor(x, dtype=torch.float32)},\\n                (\\\"node_a\\\", \\\"love\\\", \\\"node_a\\\"): {\\n                    \\\"edge_index\\\": torch.tensor(\\n                        edge_index_dict[\\\"type1\\\"], dtype=torch.long\\n                    )\\n                },\\n                (\\\"node_a\\\", \\\"hate\\\", \\\"node_a\\\"): {\\n                    \\\"edge_index\\\": torch.tensor(\\n                        edge_index_dict[\\\"type2\\\"], dtype=torch.long\\n                    )\\n                },\\n            }\\n        )\\n        for x, y in zip(X, Y)\\n    ]\\n    return data_list\\n\\n\\nclass SyntheticDataModule(pl.LightningDataModule):\\n    def __init__(self, batch_size=100, **kwargs):\\n        super().__init__()\\n        self.batch_size = batch_size\\n\\n        full_data_list = get_data_list(**kwargs)\\n        self.train_data_list, self.test_data_list = train_test_split(\\n            full_data_list, test_size=0.2, shuffle=True\\n        )\\n        self.static_edge_index_dict = full_data_list[0].edge_index_dict\\n\\n    def train_dataloader(self):\\n        return DataLoader(self.train_data_list, batch_size=self.batch_size)\\n\\n    def test_dataloader(self):\\n        return DataLoader(self.test_data_list, batch_size=self.batch_size)\\n\\n    def get_single_sample(self):\\n        return self.train_data_list[0]\\n\\n\\nclass HeteroGNNArch(torch.nn.Module):\\n    def __init__(self, hidden_channels, out_channels, num_layers, number_of_labels=2):\\n        super().__init__()\\n\\n        self.convs = torch.nn.ModuleList()\\n        for _ in range(num_layers):\\n            conv = pygnn.HeteroConv(\\n                {\\n                    (\\\"node_a\\\", \\\"love\\\", \\\"node_a\\\"): pygnn.GCNConv(-1, hidden_channels),\\n                    (\\\"node_a\\\", \\\"hate\\\", \\\"node_a\\\"): pygnn.SAGEConv(\\n                        (-1, -1), hidden_channels\\n                    ),\\n                },\\n                aggr=\\\"sum\\\",\\n            )\\n            self.convs.append(conv)\\n\\n        self.lin = pygnn.Linear(-1, hidden_channels)\\n\\n        self.dense = torch.nn.LazyLinear(number_of_labels)\\n        self.softmax = torch.nn.Softmax(dim=0)\\n\\n    def forward(self, x_dict, edge_index_dict, batch_dict: Optional[Dict] = None):\\n        # todo: make sure x_dict.keys() is orderd like batch_vector\\n        for conv in self.convs:\\n            x_dict = conv(x_dict, edge_index_dict)\\n            x_dict = {key: x.relu() for key, x in x_dict.items()}\\n        x_dict = {key: self.lin(xx) for key, xx in x_dict.items()}\\n        x_dict_batched = {\\n            key: torch.flatten(to_dense_batch(xx, b_vec)[0], start_dim=1)\\n            for (key, xx), b_vec in zip(x_dict.items(), batch_dict.values())\\n        }\\n        x = torch.cat(list(x_dict_batched.values()))\\n        x = self.dense(x)\\n        output = self.softmax(x)\\n        # TODO:  try a layer that recive a dictonery(diffrent kind of nodes)_\\n        return output\\n\\n\\nclass RamSimpleModel(pl.LightningModule):\\n    def __init__(self, model_arch: torch.nn.Module, **kwargs):\\n        super().__init__()\\n        self.model_arch = model_arch\\n\\n        self.static_edge_index_dict = None\\n\\n    def training_step(self, batch, batch_idx):\\n        x_dict, y_t, edge_index_dict, batch_dict = (\\n            batch.x_dict,\\n            batch.y,\\n            batch.edge_index_dict,\\n            batch.batch_dict,\\n        )\\n        # edge_index_dict = {key:torch.tile(x,[y.size()[0],1,1]) for key,x in self.static_edge_index_dict.items()}\\n        pred = self.model_arch(x_dict, edge_index_dict, batch_dict)\\n        y_t_onehot = torch.tensor(\\n            torch.nn.functional.one_hot(y_t, 2), dtype=torch.float32\\n        )\\n        train_loss = torch.nn.functional.binary_cross_entropy(pred, y_t_onehot)\\n        correct = pred.argmax(dim=1).eq(y_t).sum().item()\\n        total = pred.size()[0]\\n\\n        logs = {\\\"train_loss\\\": train_loss}\\n        batch_logs_dictionary = {\\n            \\\"loss\\\": train_loss,\\n            \\\"log\\\": logs,\\n            \\\"correct\\\": correct,\\n            \\\"total\\\": total,\\n        }\\n        return batch_logs_dictionary\\n\\n    def training_epoch_end(self, outputs) -> None:\\n        avg_loss = torch.stack([xx[\\\"loss\\\"] for xx in outputs]).mean()\\n        correct = sum([x[\\\"correct\\\"] for x in outputs])\\n        total = sum([x[\\\"total\\\"] for x in outputs])\\n\\n        self.logger.experiment.add_scalar(\\\"Loss/Train\\\", avg_loss, self.current_epoch)\\n        self.logger.experiment.add_scalar(\\n            \\\"Accuracy/Train\\\", correct / total, self.current_epoch\\n        )\\n\\n    def configure_optimizers(self):\\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\\n        return optimizer\\n\\n    def lazy_initialize(self, sample):\\n        # x_dict = {key:torch.tile(x,[batch_size,1,1]) for key,x in sample.x_dict.items()}\\n        # edge_index_dict = {key:torch.tile(x,[batch_size,1,1]) for key,x in sample.edge_index_dict.items()}\\n        batch_dict = {\\n            key: torch.zeros(xx.size()[0], dtype=torch.long)\\n            for key, xx in sample.x_dict.items()\\n        }\\n        with torch.no_grad():  # Initialize lazy modules.\\n            _ = self.model_arch(\\n                sample.x_dict, sample.edge_index_dict, batch_dict=batch_dict\\n            )\\n        self.logger.experiment.add_graph(\\n            RamSimpleModel(self.model_arch),\\n            (sample.x_dict, sample.edge_index_dict, batch_dict),\\n        )\\n\\n    def set_static_edge_index(self, edge_index):\\n        self.static_edge_index_dict = edge_index\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": "<IPython.core.display.Javascript object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "def synthesize_mixing_vector(length, random_state=46, **kwargs):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        length (int) : length = number_of_nodes*number of features\n",
    "        random_state (int): seed for random shuffling\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray mixing vector\n",
    "    \"\"\"\n",
    "    indices = np.arange(length)\n",
    "    return shuffle(indices, random_state=random_state)\n",
    "\n",
    "\n",
    "def generate_positive_negative_data(length, total_samples=10000, **kwargs):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        length (int) : length = number_of_nodes*number of features\n",
    "        total_samples (int): total number of instances\n",
    "        **kwargs:\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray,np.ndarray] for positive and negative data\n",
    "    \"\"\"\n",
    "    mixing_vec_pos = synthesize_mixing_vector(length, **kwargs)\n",
    "    A_pos = np.random.randn(length, length)\n",
    "    cov_pos = np.matmul(A_pos.T, A_pos)\n",
    "    mean_pos = np.amax(np.abs(cov_pos.flatten())) * (1 + np.random.randn(length) * 0.25)\n",
    "    raw_samples_pos = np.random.multivariate_normal(mean_pos, cov_pos, size=total_samples)\n",
    "    samples_pos = raw_samples_pos[:, mixing_vec_pos]\n",
    "\n",
    "    mixing_vec_neg = synthesize_mixing_vector(length, **kwargs)\n",
    "    A_neg = np.random.randn(length, length)\n",
    "    cov_neg = np.matmul(A_neg.T, A_neg)\n",
    "    mean_neg = np.amax(np.abs(cov_neg.flatten())) * (-1 + np.random.randn(length) * 0.25)\n",
    "    raw_samples_neg = np.random.multivariate_normal(mean_neg, cov_neg, size=total_samples)\n",
    "    samples_neg = raw_samples_neg[:, mixing_vec_neg]\n",
    "    return samples_pos, samples_neg\n",
    "\n",
    "\n",
    "# noinspection PyPep8Naming\n",
    "def get_data_list(number_of_nodes=15, number_of_features=5, total_samples=10000, **kwargs):\n",
    "    X_p, X_n = generate_positive_negative_data(number_of_nodes * number_of_features, total_samples)\n",
    "    Y = [0] * X_n.shape[0] + [1] * X_p.shape[0]\n",
    "    X_p = np.reshape(X_p, (total_samples, number_of_nodes, number_of_features))\n",
    "    X_n = np.reshape(X_n, (total_samples, number_of_nodes, number_of_features))\n",
    "    X = np.concatenate([X_n, X_p], axis=0)\n",
    "    edge_index_dict = {\"type1\": [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    "                                 [0, 0, 0, 0, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4]],\n",
    "                       \"type2\": [[0, 0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14],\n",
    "                                 [1, 5, 7, 7, 7, 9, 6, 7, 9, 14, 11, 6, 7, 7, 13]]}\n",
    "    data_list = [HeteroData(\n",
    "        {\"y\": torch.tensor(y, dtype=torch.long),\n",
    "         \"node_a\": {\"x\": torch.tensor(x, dtype=torch.float32)},\n",
    "         (\"node_a\", \"love\", \"node_a\"): {\"edge_index\": torch.tensor(edge_index_dict[\"type1\"], dtype=torch.long)},\n",
    "         (\"node_a\", \"hate\", \"node_a\"): {\"edge_index\": torch.tensor(edge_index_dict[\"type2\"], dtype=torch.long)}\n",
    "         })\n",
    "        for x, y in zip(X, Y)]\n",
    "    return data_list\n",
    "\n",
    "\n",
    "class SyntheticDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=100, **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        full_data_list = get_data_list(**kwargs)\n",
    "        self.train_data_list, self.test_data_list = train_test_split(full_data_list, test_size=0.2, shuffle=True)\n",
    "        self.static_edge_index_dict = full_data_list[0].edge_index_dict\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data_list, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data_list, batch_size=self.batch_size)\n",
    "\n",
    "    def get_single_sample(self):\n",
    "        return self.train_data_list[0]\n",
    "\n",
    "\n",
    "class HeteroGNNArch(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers, number_of_labels=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = pygnn.HeteroConv({\n",
    "                (\"node_a\", \"love\", \"node_a\"): pygnn.GCNConv(-1, hidden_channels),\n",
    "                (\"node_a\", \"hate\", \"node_a\"): pygnn.SAGEConv((-1, -1), hidden_channels),\n",
    "            }, aggr='sum')\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.lin = pygnn.Linear(-1, hidden_channels)\n",
    "\n",
    "        self.dense = torch.nn.LazyLinear(number_of_labels)\n",
    "        self.softmax = torch.nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, batch_dict: Optional[Dict] = None):\n",
    "        # todo: make sure x_dict.keys() is orderd like batch_vector\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "        x_dict = {key: self.lin(xx) for key, xx in x_dict.items()}\n",
    "        x_dict_batched = {key: torch.flatten(to_dense_batch(xx, b_vec)[0], start_dim=1) for (key, xx), b_vec in\n",
    "                          zip(x_dict.items(), batch_dict.values())}\n",
    "        x = torch.cat(list(x_dict_batched.values()))\n",
    "        x = self.dense(x)\n",
    "        output = self.softmax(x)\n",
    "        # TODO:  try a layer that recive a dictonery(diffrent kind of nodes)_\n",
    "        return output\n",
    "\n",
    "\n",
    "class RamSimpleModel(pl.LightningModule):\n",
    "    def __init__(self, model_arch: torch.nn.Module, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model_arch = model_arch\n",
    "        self.static_edge_index_dict = None\n",
    "        self.is_computational_graph_saved = False\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_dict, y_t, edge_index_dict, batch_dict = batch.x_dict, batch.y, batch.edge_index_dict, batch.batch_dict\n",
    "        # edge_index_dict = {key:torch.tile(x,[y.size()[0],1,1]) for key,x in self.static_edge_index_dict.items()}\n",
    "        pred = self.model_arch(x_dict, edge_index_dict, batch_dict)\n",
    "        y_t_onehot = torch.tensor(torch.nn.functional.one_hot(y_t, 2), dtype=torch.float32)\n",
    "        train_loss = torch.nn.functional.binary_cross_entropy(pred, y_t_onehot)\n",
    "        correct = pred.argmax(dim=1).eq(y_t).sum().item()\n",
    "        total = pred.size()[0]\n",
    "\n",
    "        logs = {\"train_loss\": train_loss}\n",
    "        batch_logs_dictionary = {\n",
    "            \"loss\": train_loss,\n",
    "            \"log\": logs,\n",
    "            \"correct\": correct,\n",
    "            \"total\": total,\n",
    "        }\n",
    "        return batch_logs_dictionary\n",
    "\n",
    "    def training_epoch_end(self, outputs) -> None:\n",
    "        avg_loss = torch.stack([xx['loss'] for xx in outputs]).mean()\n",
    "        correct = sum([x[\"correct\"] for x in outputs])\n",
    "        total = sum([x[\"total\"] for x in outputs])\n",
    "\n",
    "        self.logger.experiment.add_scalar(\"Loss/Train\", avg_loss, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"Accuracy/Train\", correct / total, self.current_epoch)\n",
    "        self.save_computational_graph()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def lazy_initialize(self, sample):\n",
    "        # x_dict = {key:torch.tile(x,[batch_size,1,1]) for key,x in sample.x_dict.items()}\n",
    "        # edge_index_dict = {key:torch.tile(x,[batch_size,1,1]) for key,x in sample.edge_index_dict.items()}\n",
    "        batch_dict = {key: torch.zeros(xx.size()[0], dtype=torch.long) for key, xx in sample.x_dict.items()}\n",
    "        with torch.no_grad():  # Initialize lazy modules.\n",
    "            _ = self.model_arch(sample.x_dict, sample.edge_index_dict, batch_dict=batch_dict)\n",
    "        # self.logger.experiment.add_graph(RamSimpleModel(self.model_arch),(sample.x_dict, sample.edge_index_dict,batch_dict))\n",
    "\n",
    "\n",
    "    def train(self, sample):\n",
    "        self.lazy_initialize(sample)\n",
    "\n",
    "    # def save_computational_graph(self,sample):\n",
    "    #     batch_dict = {key: torch.zeros(xx.size()[0], dtype=torch.long) for key, xx in sample.x_dict.items()}\n",
    "        # self.logger.experiment.add_graph(RamSimpleModel(self.model_arch),(sample.x_dict, sample.edge_index_dict,batch_dict))\n",
    "\n",
    "\n",
    "    def set_static_edge_index(self, edge_index):\n",
    "        self.static_edge_index_dict = edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ramshirazi/opt/anaconda3/envs/ram_dev/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'experiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m model_arch \u001b[38;5;241m=\u001b[39m HeteroGNNArch(hidden_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      4\u001b[0m ram_model \u001b[38;5;241m=\u001b[39m RamSimpleModel(model_arch\u001b[38;5;241m=\u001b[39mmodel_arch)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mram_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msynthetic_data_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m ram_model\u001b[38;5;241m.\u001b[39mset_static_edge_index(synthetic_data_module\u001b[38;5;241m.\u001b[39mstatic_edge_index_dict)\n\u001b[1;32m      8\u001b[0m logs_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m((Path\u001b[38;5;241m.\u001b[39mcwd()\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mabsolute())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mRamSimpleModel.lazy_initialize\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Initialize lazy modules.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_arch(sample\u001b[38;5;241m.\u001b[39mx_dict, sample\u001b[38;5;241m.\u001b[39medge_index_dict, batch_dict\u001b[38;5;241m=\u001b[39mbatch_dict)\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiment\u001b[49m\u001b[38;5;241m.\u001b[39madd_graph(RamSimpleModel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_arch),(sample\u001b[38;5;241m.\u001b[39mx_dict, sample\u001b[38;5;241m.\u001b[39medge_index_dict,batch_dict))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'experiment'"
     ]
    },
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 4;\n                var nbb_unformatted_code = \"# train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\\nsynthetic_data_module = SyntheticDataModule()\\nmodel_arch = HeteroGNNArch(hidden_channels=24, out_channels=2, num_layers=2)\\nram_model = RamSimpleModel(model_arch=model_arch)\\nram_model.lazy_initialize(synthetic_data_module.get_single_sample())\\nram_model.set_static_edge_index(synthetic_data_module.static_edge_index_dict)\\n\\nlogs_root = f\\\"{str((Path.cwd().parent / 'logs').absolute())}\\\"\\ntb_logger = pl.loggers.tensorboard.TensorBoardLogger(save_dir=logs_root,\\n                                                     name=datetime.now().strftime(\\\"%d-%m-%Y-%H-%M\\\"))\\ntrainer = pl.Trainer(logger=tb_logger)\\ntrainer.fit(ram_model, synthetic_data_module)\\na = 1\";\n                var nbb_formatted_code = \"# train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\\nsynthetic_data_module = SyntheticDataModule()\\nmodel_arch = HeteroGNNArch(hidden_channels=24, out_channels=2, num_layers=2)\\nram_model = RamSimpleModel(model_arch=model_arch)\\nram_model.lazy_initialize(synthetic_data_module.get_single_sample())\\nram_model.set_static_edge_index(synthetic_data_module.static_edge_index_dict)\\n\\nlogs_root = f\\\"{str((Path.cwd().parent / 'logs').absolute())}\\\"\\ntb_logger = pl.loggers.tensorboard.TensorBoardLogger(\\n    save_dir=logs_root, name=datetime.now().strftime(\\\"%d-%m-%Y-%H-%M\\\")\\n)\\ntrainer = pl.Trainer(logger=tb_logger)\\ntrainer.fit(ram_model, synthetic_data_module)\\na = 1\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": "<IPython.core.display.Javascript object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\n",
    "synthetic_data_module = SyntheticDataModule()\n",
    "model_arch = HeteroGNNArch(hidden_channels=24, out_channels=2, num_layers=2)\n",
    "ram_model = RamSimpleModel(model_arch=model_arch)\n",
    "ram_model.lazy_initialize(synthetic_data_module.get_single_sample())\n",
    "ram_model.set_static_edge_index(synthetic_data_module.static_edge_index_dict)\n",
    "\n",
    "logs_root = f\"{str((Path.cwd().parent / 'logs').absolute())}\"\n",
    "tb_logger = pl.loggers.tensorboard.TensorBoardLogger(save_dir=logs_root,\n",
    "                                                     name=datetime.now().strftime(\"%d-%m-%Y-%H-%M\"))\n",
    "trainer = pl.Trainer(logger=tb_logger)\n",
    "\n",
    "trainer.fit(ram_model, synthetic_data_module)\n",
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from nbdev.export import notebook2script;\n",
    "\n",
    "notebook2script()\n",
    "#\n",
    "# notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
