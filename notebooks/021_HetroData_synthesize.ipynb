{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp HeteroData_synthesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 1;\n                var nbb_unformatted_code = \"%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n%matplotlib inline\";\n                var nbb_formatted_code = \"%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\\n%matplotlib inline\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": "<IPython.core.display.Javascript object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% load_ext nb_black\n",
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 2;\n                var nbb_unformatted_code = \"# export\\nimport numpy as np\\nfrom datetime import datetime\\nfrom sklearn.utils import shuffle\\nfrom sklearn.model_selection import train_test_split\\nfrom typing import Tuple, Optional, Dict\\nimport pytorch_lightning as pl\\nimport torch\\nfrom torch_geometric.utils import to_dense_batch\\nfrom torch_geometric.data import HeteroData\\nimport torch_geometric.nn as pygnn\\nfrom torch_geometric.loader import DataLoader\\nfrom pathlib import Path\\nfrom clearml import Task\";\n                var nbb_formatted_code = \"# export\\nimport numpy as np\\nfrom datetime import datetime\\nfrom sklearn.utils import shuffle\\nfrom sklearn.model_selection import train_test_split\\nfrom typing import Tuple, Optional, Dict\\nimport pytorch_lightning as pl\\nimport torch\\nfrom torch_geometric.utils import to_dense_batch\\nfrom torch_geometric.data import HeteroData\\nimport torch_geometric.nn as pygnn\\nfrom torch_geometric.loader import DataLoader\\nfrom pathlib import Path\\nfrom clearml import Task\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": "<IPython.core.display.Javascript object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Tuple, Optional, Dict\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.nn as pygnn\n",
    "from torch_geometric.loader import DataLoader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 6;\n                var nbb_unformatted_code = \"#export\\ndef synthesize_mixing_vector(length, random_state=46, **kwargs):\\n    \\\"\\\"\\\"\\n\\n    Args:\\n        length (int) : length = number_of_nodes*number of features\\n        random_state (int): seed for random shuffling\\n\\n    Returns:\\n        np.ndarray mixing vector\\n    \\\"\\\"\\\"\\n    indices = np.arange(length)\\n    return shuffle(indices, random_state=random_state)\\n\\n\\ndef generate_positive_negative_data(length, total_samples=10000, **kwargs):\\n    \\\"\\\"\\\"\\n\\n    Args:\\n        length (int) : length = number_of_nodes*number of features\\n        total_samples (int): total number of instances\\n        **kwargs:\\n\\n    Returns:\\n        Tuple[np.ndarray,np.ndarray] for positive and negative data\\n    \\\"\\\"\\\"\\n    mixing_vec_pos = synthesize_mixing_vector(length, **kwargs)\\n    A_pos = np.random.randn(length, length)\\n    cov_pos = np.matmul(A_pos.T, A_pos)\\n    mean_pos = np.amax(np.abs(cov_pos.flatten())) * (1 + np.random.randn(length) * 0.25)\\n    raw_samples_pos = np.random.multivariate_normal(mean_pos, cov_pos, size=total_samples)\\n    samples_pos = raw_samples_pos[:, mixing_vec_pos]\\n\\n    mixing_vec_neg = synthesize_mixing_vector(length, **kwargs)\\n    A_neg = np.random.randn(length, length)\\n    cov_neg = np.matmul(A_neg.T, A_neg)\\n    mean_neg = np.amax(np.abs(cov_neg.flatten())) * (-1 + np.random.randn(length) * 0.25)\\n    raw_samples_neg = np.random.multivariate_normal(mean_neg, cov_neg, size=total_samples)\\n    samples_neg = raw_samples_neg[:, mixing_vec_neg]\\n    return samples_pos, samples_neg\\n\\n\\n# noinspection PyPep8Naming\\ndef get_data_list(number_of_nodes=15, number_of_features=5, total_samples=10000, **kwargs):\\n    X_p, X_n = generate_positive_negative_data(number_of_nodes * number_of_features, total_samples)\\n    Y = [0] * X_n.shape[0] + [1] * X_p.shape[0]\\n    X_p = np.reshape(X_p, (total_samples, number_of_nodes, number_of_features))\\n    X_n = np.reshape(X_n, (total_samples, number_of_nodes, number_of_features))\\n    X = np.concatenate([X_n, X_p], axis=0)\\n    edge_index_dict = {\\\"type1\\\": [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\\n                                 [0, 0, 0, 0, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4]],\\n                       \\\"type2\\\": [[0, 0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14],\\n                                 [1, 5, 7, 7, 7, 9, 6, 7, 9, 14, 11, 6, 7, 7, 13]]}\\n    data_list = [HeteroData(\\n        {\\\"y\\\": torch.tensor(y, dtype=torch.long),\\n         \\\"node_a\\\": {\\\"x\\\": torch.tensor(x, dtype=torch.float32)},\\n         (\\\"node_a\\\", \\\"love\\\", \\\"node_a\\\"): {\\\"edge_index\\\": torch.tensor(edge_index_dict[\\\"type1\\\"], dtype=torch.long)},\\n         (\\\"node_a\\\", \\\"hate\\\", \\\"node_a\\\"): {\\\"edge_index\\\": torch.tensor(edge_index_dict[\\\"type2\\\"], dtype=torch.long)}\\n         })\\n        for x, y in zip(X, Y)]\\n    return data_list\\n\\n\\nclass SyntheticDataModule(pl.LightningDataModule):\\n    def __init__(self, batch_size=100, **kwargs):\\n        super().__init__()\\n        self.batch_size = batch_size\\n\\n        full_data_list = get_data_list(**kwargs)\\n        self.train_data_list, self.test_data_list = train_test_split(full_data_list, test_size=0.2, shuffle=True)\\n        self.static_edge_index_dict = full_data_list[0].edge_index_dict\\n\\n    def train_dataloader(self):\\n        return DataLoader(self.train_data_list, batch_size=self.batch_size)\\n\\n    def test_dataloader(self):\\n        return DataLoader(self.test_data_list, batch_size=self.batch_size)\\n\\n    def get_single_sample(self):\\n        return self.train_data_list[0]\\n\\n\\nclass HeteroGNNArch(torch.nn.Module):\\n    def __init__(self, hidden_channels, out_channels, num_layers, number_of_labels=2):\\n        super().__init__()\\n\\n        self.convs = torch.nn.ModuleList()\\n        for _ in range(num_layers):\\n            conv = pygnn.HeteroConv({\\n                (\\\"node_a\\\", \\\"love\\\", \\\"node_a\\\"): pygnn.GCNConv(-1, hidden_channels),\\n                (\\\"node_a\\\", \\\"hate\\\", \\\"node_a\\\"): pygnn.SAGEConv((-1, -1), hidden_channels),\\n            }, aggr='sum')\\n            self.convs.append(conv)\\n\\n        self.lin = pygnn.Linear(-1, hidden_channels)\\n\\n        self.dense = torch.nn.LazyLinear(number_of_labels)\\n        self.softmax = torch.nn.Softmax(dim=0)\\n\\n    def forward(self, x_dict, edge_index_dict, batch_dict: Optional[Dict] = None):\\n        # todo: make sure x_dict.keys() is orderd like batch_vector\\n        for conv in self.convs:\\n            x_dict = conv(x_dict, edge_index_dict)\\n            x_dict = {key: x.relu() for key, x in x_dict.items()}\\n        x_dict = {key: self.lin(xx) for key, xx in x_dict.items()}\\n        x_dict_batched = {key: torch.flatten(to_dense_batch(xx, b_vec)[0], start_dim=1) for (key, xx), b_vec in\\n                          zip(x_dict.items(), batch_dict.values())}\\n        x = torch.cat(list(x_dict_batched.values()))\\n        x = self.dense(x)\\n        output = self.softmax(x)\\n        # TODO:  try a layer that recive a dictonery(diffrent kind of nodes)_\\n        return output\\n\\n\\nclass RamSimpleModel(pl.LightningModule):\\n    def __init__(self, model_arch: torch.nn.Module, **kwargs):\\n        super().__init__()\\n        self.model_arch = model_arch\\n        self.static_edge_index_dict = None\\n        self.is_computational_graph_saved = False\\n\\n    def training_step(self, batch, batch_idx):\\n        x_dict, y_t, edge_index_dict, batch_dict = batch.x_dict, batch.y, batch.edge_index_dict, batch.batch_dict\\n        # edge_index_dict = {key:torch.tile(x,[y.size()[0],1,1]) for key,x in self.static_edge_index_dict.items()}\\n        pred = self.model_arch(x_dict, edge_index_dict, batch_dict)\\n        y_t_onehot = torch.tensor(torch.nn.functional.one_hot(y_t, 2), dtype=torch.float32)\\n        train_loss = torch.nn.functional.binary_cross_entropy(pred, y_t_onehot)\\n        correct = pred.argmax(dim=1).eq(y_t).sum().item()\\n        total = pred.size()[0]\\n\\n        logs = {\\\"train_loss\\\": train_loss}\\n        batch_logs_dictionary = {\\n            \\\"loss\\\": train_loss,\\n            \\\"log\\\": logs,\\n            \\\"correct\\\": correct,\\n            \\\"total\\\": total,\\n        }\\n        return batch_logs_dictionary\\n\\n    def training_epoch_end(self, outputs) -> None:\\n        avg_loss = torch.stack([xx['loss'] for xx in outputs]).mean()\\n        correct = sum([x[\\\"correct\\\"] for x in outputs])\\n        total = sum([x[\\\"total\\\"] for x in outputs])\\n\\n        self.logger.experiment.add_scalar(\\\"Loss/Train\\\", avg_loss, self.current_epoch)\\n        self.logger.experiment.add_scalar(\\\"Accuracy/Train\\\", correct / total, self.current_epoch)\\n        # self.save_computational_graph()\\n\\n    def configure_optimizers(self):\\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\\n        return optimizer\\n\\n    def lazy_initialize(self, sample):\\n        # x_dict = {key:torch.tile(x,[batch_size,1,1]) for key,x in sample.x_dict.items()}\\n        # edge_index_dict = {key:torch.tile(x,[batch_size,1,1]) for key,x in sample.edge_index_dict.items()}\\n        batch_dict = {key: torch.zeros(xx.size()[0], dtype=torch.long) for key, xx in sample.x_dict.items()}\\n        with torch.no_grad():  # Initialize lazy modules.\\n            _ = self.model_arch(sample.x_dict, sample.edge_index_dict, batch_dict=batch_dict)\\n        # self.logger.experiment.add_graph(RamSimpleModel(self.model_arch),(sample.x_dict, sample.edge_index_dict,batch_dict))\\n\\n    # def save_computational_graph(self,sample):\\n    #     batch_dict = {key: torch.zeros(xx.size()[0], dtype=torch.long) for key, xx in sample.x_dict.items()}\\n        # self.logger.experiment.add_graph(RamSimpleModel(self.model_arch),(sample.x_dict, sample.edge_index_dict,batch_dict))\\n\\n    def set_static_edge_index(self, edge_index):\\n        self.static_edge_index_dict = edge_index\";\n                var nbb_formatted_code = \"# export\\ndef synthesize_mixing_vector(length, random_state=46, **kwargs):\\n    \\\"\\\"\\\"\\n\\n    Args:\\n        length (int) : length = number_of_nodes*number of features\\n        random_state (int): seed for random shuffling\\n\\n    Returns:\\n        np.ndarray mixing vector\\n    \\\"\\\"\\\"\\n    indices = np.arange(length)\\n    return shuffle(indices, random_state=random_state)\\n\\n\\ndef generate_positive_negative_data(length, total_samples=10000, **kwargs):\\n    \\\"\\\"\\\"\\n\\n    Args:\\n        length (int) : length = number_of_nodes*number of features\\n        total_samples (int): total number of instances\\n        **kwargs:\\n\\n    Returns:\\n        Tuple[np.ndarray,np.ndarray] for positive and negative data\\n    \\\"\\\"\\\"\\n    mixing_vec_pos = synthesize_mixing_vector(length, **kwargs)\\n    A_pos = np.random.randn(length, length)\\n    cov_pos = np.matmul(A_pos.T, A_pos)\\n    mean_pos = np.amax(np.abs(cov_pos.flatten())) * (1 + np.random.randn(length) * 0.25)\\n    raw_samples_pos = np.random.multivariate_normal(\\n        mean_pos, cov_pos, size=total_samples\\n    )\\n    samples_pos = raw_samples_pos[:, mixing_vec_pos]\\n\\n    mixing_vec_neg = synthesize_mixing_vector(length, **kwargs)\\n    A_neg = np.random.randn(length, length)\\n    cov_neg = np.matmul(A_neg.T, A_neg)\\n    mean_neg = np.amax(np.abs(cov_neg.flatten())) * (\\n        -1 + np.random.randn(length) * 0.25\\n    )\\n    raw_samples_neg = np.random.multivariate_normal(\\n        mean_neg, cov_neg, size=total_samples\\n    )\\n    samples_neg = raw_samples_neg[:, mixing_vec_neg]\\n    return samples_pos, samples_neg\\n\\n\\n# noinspection PyPep8Naming\\ndef get_data_list(\\n    number_of_nodes=15, number_of_features=5, total_samples=10000, **kwargs\\n):\\n    X_p, X_n = generate_positive_negative_data(\\n        number_of_nodes * number_of_features, total_samples\\n    )\\n    Y = [0] * X_n.shape[0] + [1] * X_p.shape[0]\\n    X_p = np.reshape(X_p, (total_samples, number_of_nodes, number_of_features))\\n    X_n = np.reshape(X_n, (total_samples, number_of_nodes, number_of_features))\\n    X = np.concatenate([X_n, X_p], axis=0)\\n    edge_index_dict = {\\n        \\\"type1\\\": [\\n            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\\n            [0, 0, 0, 0, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4],\\n        ],\\n        \\\"type2\\\": [\\n            [0, 0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14],\\n            [1, 5, 7, 7, 7, 9, 6, 7, 9, 14, 11, 6, 7, 7, 13],\\n        ],\\n    }\\n    data_list = [\\n        HeteroData(\\n            {\\n                \\\"y\\\": torch.tensor(y, dtype=torch.long),\\n                \\\"node_a\\\": {\\\"x\\\": torch.tensor(x, dtype=torch.float32)},\\n                (\\\"node_a\\\", \\\"love\\\", \\\"node_a\\\"): {\\n                    \\\"edge_index\\\": torch.tensor(\\n                        edge_index_dict[\\\"type1\\\"], dtype=torch.long\\n                    )\\n                },\\n                (\\\"node_a\\\", \\\"hate\\\", \\\"node_a\\\"): {\\n                    \\\"edge_index\\\": torch.tensor(\\n                        edge_index_dict[\\\"type2\\\"], dtype=torch.long\\n                    )\\n                },\\n            }\\n        )\\n        for x, y in zip(X, Y)\\n    ]\\n    return data_list\\n\\n\\nclass SyntheticDataModule(pl.LightningDataModule):\\n    def __init__(self, batch_size=100, **kwargs):\\n        super().__init__()\\n        self.batch_size = batch_size\\n\\n        full_data_list = get_data_list(**kwargs)\\n        self.train_data_list, self.test_data_list = train_test_split(\\n            full_data_list, test_size=0.2, shuffle=True\\n        )\\n        self.static_edge_index_dict = full_data_list[0].edge_index_dict\\n\\n    def train_dataloader(self):\\n        return DataLoader(self.train_data_list, batch_size=self.batch_size)\\n\\n    def test_dataloader(self):\\n        return DataLoader(self.test_data_list, batch_size=self.batch_size)\\n\\n    def get_single_sample(self):\\n        return self.train_data_list[0]\\n\\n\\nclass HeteroGNNArch(torch.nn.Module):\\n    def __init__(self, hidden_channels, out_channels, num_layers, number_of_labels=2):\\n        super().__init__()\\n\\n        self.convs = torch.nn.ModuleList()\\n        for _ in range(num_layers):\\n            conv = pygnn.HeteroConv(\\n                {\\n                    (\\\"node_a\\\", \\\"love\\\", \\\"node_a\\\"): pygnn.GCNConv(-1, hidden_channels),\\n                    (\\\"node_a\\\", \\\"hate\\\", \\\"node_a\\\"): pygnn.SAGEConv(\\n                        (-1, -1), hidden_channels\\n                    ),\\n                },\\n                aggr=\\\"sum\\\",\\n            )\\n            self.convs.append(conv)\\n\\n        self.lin = pygnn.Linear(-1, hidden_channels)\\n\\n        self.dense = torch.nn.LazyLinear(number_of_labels)\\n        self.softmax = torch.nn.Softmax(dim=0)\\n\\n    def forward(self, x_dict, edge_index_dict, batch_dict: Optional[Dict] = None):\\n        # todo: make sure x_dict.keys() is orderd like batch_vector\\n        for conv in self.convs:\\n            x_dict = conv(x_dict, edge_index_dict)\\n            x_dict = {key: x.relu() for key, x in x_dict.items()}\\n        x_dict = {key: self.lin(xx) for key, xx in x_dict.items()}\\n        x_dict_batched = {\\n            key: torch.flatten(to_dense_batch(xx, b_vec)[0], start_dim=1)\\n            for (key, xx), b_vec in zip(x_dict.items(), batch_dict.values())\\n        }\\n        x = torch.cat(list(x_dict_batched.values()))\\n        x = self.dense(x)\\n        output = self.softmax(x)\\n        # TODO:  try a layer that recive a dictonery(diffrent kind of nodes)_\\n        return output\\n\\n\\nclass RamSimpleModel(pl.LightningModule):\\n    def __init__(self, model_arch: torch.nn.Module, **kwargs):\\n        super().__init__()\\n        self.model_arch = model_arch\\n        self.static_edge_index_dict = None\\n        self.is_computational_graph_saved = False\\n\\n    def training_step(self, batch, batch_idx):\\n        x_dict, y_t, edge_index_dict, batch_dict = (\\n            batch.x_dict,\\n            batch.y,\\n            batch.edge_index_dict,\\n            batch.batch_dict,\\n        )\\n        # edge_index_dict = {key:torch.tile(x,[y.size()[0],1,1]) for key,x in self.static_edge_index_dict.items()}\\n        pred = self.model_arch(x_dict, edge_index_dict, batch_dict)\\n        y_t_onehot = torch.tensor(\\n            torch.nn.functional.one_hot(y_t, 2), dtype=torch.float32\\n        )\\n        train_loss = torch.nn.functional.binary_cross_entropy(pred, y_t_onehot)\\n        correct = pred.argmax(dim=1).eq(y_t).sum().item()\\n        total = pred.size()[0]\\n\\n        logs = {\\\"train_loss\\\": train_loss}\\n        batch_logs_dictionary = {\\n            \\\"loss\\\": train_loss,\\n            \\\"log\\\": logs,\\n            \\\"correct\\\": correct,\\n            \\\"total\\\": total,\\n        }\\n        return batch_logs_dictionary\\n\\n    def training_epoch_end(self, outputs) -> None:\\n        avg_loss = torch.stack([xx[\\\"loss\\\"] for xx in outputs]).mean()\\n        correct = sum([x[\\\"correct\\\"] for x in outputs])\\n        total = sum([x[\\\"total\\\"] for x in outputs])\\n\\n        self.logger.experiment.add_scalar(\\\"Loss/Train\\\", avg_loss, self.current_epoch)\\n        self.logger.experiment.add_scalar(\\n            \\\"Accuracy/Train\\\", correct / total, self.current_epoch\\n        )\\n        # self.save_computational_graph()\\n\\n    def configure_optimizers(self):\\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\\n        return optimizer\\n\\n    def lazy_initialize(self, sample):\\n        # x_dict = {key:torch.tile(x,[batch_size,1,1]) for key,x in sample.x_dict.items()}\\n        # edge_index_dict = {key:torch.tile(x,[batch_size,1,1]) for key,x in sample.edge_index_dict.items()}\\n        batch_dict = {\\n            key: torch.zeros(xx.size()[0], dtype=torch.long)\\n            for key, xx in sample.x_dict.items()\\n        }\\n        with torch.no_grad():  # Initialize lazy modules.\\n            _ = self.model_arch(\\n                sample.x_dict, sample.edge_index_dict, batch_dict=batch_dict\\n            )\\n        # self.logger.experiment.add_graph(RamSimpleModel(self.model_arch),(sample.x_dict, sample.edge_index_dict,batch_dict))\\n\\n    # def save_computational_graph(self,sample):\\n    #     batch_dict = {key: torch.zeros(xx.size()[0], dtype=torch.long) for key, xx in sample.x_dict.items()}\\n    # self.logger.experiment.add_graph(RamSimpleModel(self.model_arch),(sample.x_dict, sample.edge_index_dict,batch_dict))\\n\\n    def set_static_edge_index(self, edge_index):\\n        self.static_edge_index_dict = edge_index\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": "<IPython.core.display.Javascript object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#export\n",
    "def synthesize_mixing_vector(length, random_state=46, **kwargs):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        length (int) : length = number_of_nodes*number of features\n",
    "        random_state (int): seed for random shuffling\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray mixing vector\n",
    "    \"\"\"\n",
    "    indices = np.arange(length)\n",
    "    return shuffle(indices, random_state=random_state)\n",
    "\n",
    "\n",
    "def generate_positive_negative_data(length, total_samples=1000, **kwargs):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        length (int) : length = number_of_nodes*number of features\n",
    "        total_samples (int): total number of instances\n",
    "        **kwargs:\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray,np.ndarray] for positive and negative data\n",
    "    \"\"\"\n",
    "    mixing_vec = synthesize_mixing_vector(length, **kwargs)\n",
    "\n",
    "    A_pos = np.random.randn(length, length)\n",
    "    cov_pos = np.matmul(A_pos.T, A_pos)\n",
    "    mean_pos = np.amax(np.abs(cov_pos.flatten())) * (0.1 + np.random.randn(length))\n",
    "    raw_samples_pos = np.random.multivariate_normal(mean_pos, cov_pos, size=total_samples)\n",
    "    samples_pos = raw_samples_pos[:, mixing_vec]\n",
    "\n",
    "    A_neg = np.random.randn(length, length)\n",
    "    cov_neg = np.matmul(A_neg.T, A_neg)\n",
    "    mean_neg = np.amax(np.abs(cov_neg.flatten())) * (-0.1 + np.random.randn(length))\n",
    "    raw_samples_neg = np.random.multivariate_normal(mean_neg, cov_neg, size=total_samples)\n",
    "    samples_neg = raw_samples_neg[:, mixing_vec]\n",
    "    return samples_pos, samples_neg\n",
    "\n",
    "\n",
    "# noinspection PyPep8Naming\n",
    "def get_data_list(number_of_nodes=15, number_of_features=5, total_samples=10000, **kwargs):\n",
    "    X_p, X_n = generate_positive_negative_data(number_of_nodes * number_of_features, total_samples)\n",
    "    Y = [0] * X_n.shape[0] + [1] * X_p.shape[0]\n",
    "    X_p = np.reshape(X_p, (total_samples, number_of_nodes, number_of_features))\n",
    "    X_n = np.reshape(X_n, (total_samples, number_of_nodes, number_of_features))\n",
    "    X = np.concatenate([X_n, X_p], axis=0)\n",
    "    edge_index_dict = {\"type1\": [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    "                                 [0, 0, 0, 0, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4]],\n",
    "                       \"type2\": [[0, 0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14],\n",
    "                                 [1, 5, 7, 7, 7, 9, 6, 7, 9, 14, 11, 6, 7, 7, 13]]}\n",
    "    data_list = [HeteroData(\n",
    "        {\"y\": torch.tensor(y, dtype=torch.long),\n",
    "         \"node_a\": {\"x\": torch.tensor(x, dtype=torch.float32)},\n",
    "         (\"node_a\", \"love\", \"node_a\"): {\"edge_index\": torch.tensor(edge_index_dict[\"type1\"], dtype=torch.long)},\n",
    "         (\"node_a\", \"hate\", \"node_a\"): {\"edge_index\": torch.tensor(edge_index_dict[\"type2\"], dtype=torch.long)}\n",
    "         })\n",
    "        for x, y in zip(X, Y)]\n",
    "    return data_list\n",
    "\n",
    "\n",
    "class SyntheticDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=100, **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        full_data_list = get_data_list(**kwargs)\n",
    "        self.train_data_list, self.test_data_list = train_test_split(full_data_list, test_size=0.2, shuffle=True)\n",
    "        self.static_edge_index_dict = full_data_list[0].edge_index_dict\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data_list, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data_list, batch_size=self.batch_size)\n",
    "\n",
    "    def get_single_sample(self):\n",
    "        return self.train_data_list[0]\n",
    "\n",
    "\n",
    "class HeteroGNNArch(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_layers, number_of_labels=2, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = pygnn.HeteroConv({\n",
    "                (\"node_a\", \"love\", \"node_a\"): pygnn.GCNConv(-1, hidden_channels),\n",
    "                (\"node_a\", \"hate\", \"node_a\"): pygnn.SAGEConv((-1, -1), hidden_channels),\n",
    "            }, aggr='sum')\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.lin = pygnn.Linear(-1, hidden_channels)\n",
    "\n",
    "        self.dense = torch.nn.LazyLinear(number_of_labels)\n",
    "        self.softmax = torch.nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, batch_dict: Optional[Dict] = None):\n",
    "        # todo: make sure x_dict.keys() is orderd like batch_vector\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "        x_dict = {key: self.lin(xx) for key, xx in x_dict.items()}\n",
    "        x_dict_batched = {key: torch.flatten(to_dense_batch(xx, b_vec)[0], start_dim=1) for (key, xx), b_vec in\n",
    "                          zip(x_dict.items(), batch_dict.values())}\n",
    "        x = torch.cat(list(x_dict_batched.values()))\n",
    "        x = self.dense(x)\n",
    "        output = self.softmax(x)\n",
    "        # TODO:  try a layer that recive a dictonery(diffrent kind of nodes)_\n",
    "        return output\n",
    "\n",
    "\n",
    "class RamSimpleModel(pl.LightningModule):\n",
    "    def __init__(self, model_arch: torch.nn.Module, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model_arch = model_arch\n",
    "        self.static_edge_index_dict = None\n",
    "        self.is_computational_graph_saved = False\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_dict, y_t, edge_index_dict, batch_dict = batch.x_dict, batch.y, batch.edge_index_dict, batch.batch_dict\n",
    "        # edge_index_dict = {key:torch.tile(x,[y.size()[0],1,1]) for key,x in self.static_edge_index_dict.items()}\n",
    "        pred = self.model_arch(x_dict, edge_index_dict, batch_dict)\n",
    "        y_t_onehot = torch.tensor(torch.nn.functional.one_hot(y_t, 2), dtype=torch.float32)\n",
    "        train_loss = torch.nn.functional.binary_cross_entropy(pred, y_t_onehot)\n",
    "        correct = pred.argmax(dim=1).eq(y_t).sum().item()\n",
    "        total = pred.size()[0]\n",
    "\n",
    "        logs = {\"train_loss\": train_loss}\n",
    "        batch_logs_dictionary = {\n",
    "            \"loss\": train_loss,\n",
    "            \"log\": logs,\n",
    "            \"correct\": correct,\n",
    "            \"total\": total,\n",
    "        }\n",
    "        return batch_logs_dictionary\n",
    "\n",
    "    def training_epoch_end(self, outputs) -> None:\n",
    "        avg_loss = torch.stack([xx['loss'] for xx in outputs]).mean()\n",
    "        correct = sum([x[\"correct\"] for x in outputs])\n",
    "        total = sum([x[\"total\"] for x in outputs])\n",
    "\n",
    "        self.logger.experiment.add_scalar(\"Loss/Train\", avg_loss, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar(\"Accuracy/Train\", correct / total, self.current_epoch)\n",
    "        # self.save_computational_graph()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def lazy_initialize(self, sample):\n",
    "        # x_dict = {key:torch.tile(x,[batch_size,1,1]) for key,x in sample.x_dict.items()}\n",
    "        # edge_index_dict = {key:torch.tile(x,[batch_size,1,1]) for key,x in sample.edge_index_dict.items()}\n",
    "        batch_dict = {key: torch.zeros(xx.size()[0], dtype=torch.long) for key, xx in sample.x_dict.items()}\n",
    "        with torch.no_grad():  # Initialize lazy modules.\n",
    "            _ = self.model_arch(sample.x_dict, sample.edge_index_dict, batch_dict=batch_dict)\n",
    "        # self.logger.experiment.add_graph(RamSimpleModel(self.model_arch),(sample.x_dict, sample.edge_index_dict,batch_dict))\n",
    "\n",
    "    # def save_computational_graph(self,sample):\n",
    "    #     batch_dict = {key: torch.zeros(xx.size()[0], dtype=torch.long) for key, xx in sample.x_dict.items()}\n",
    "    # self.logger.experiment.add_graph(RamSimpleModel(self.model_arch),(sample.x_dict, sample.edge_index_dict,batch_dict))\n",
    "\n",
    "    def set_static_edge_index(self, edge_index):\n",
    "        self.static_edge_index_dict = edge_index\n",
    "\n",
    "\n",
    "def run_simulation(simulation_name, synthetic_data_module=None,max_epochs=100 ,**kwargs):\n",
    "    if synthetic_data_module is None:\n",
    "        synthetic_data_module = SyntheticDataModule(**kwargs)\n",
    "    model_arch = HeteroGNNArch(**kwargs)\n",
    "    ram_model = RamSimpleModel(model_arch=model_arch)\n",
    "    ram_model.lazy_initialize(synthetic_data_module.get_single_sample())\n",
    "    ram_model.set_static_edge_index(synthetic_data_module.static_edge_index_dict)\n",
    "\n",
    "    logs_root = f\"{str((Path.cwd().parent / 'logs').absolute())}\"\n",
    "    tb_logger = pl.loggers.tensorboard.TensorBoardLogger(save_dir=logs_root, name=simulation_name)\n",
    "    trainer = pl.Trainer(logger=tb_logger,max_epochs=100)\n",
    "    trainer.fit(ram_model, synthetic_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from nbdev.export import notebook2script;\n",
    "\n",
    "notebook2script()\n",
    "#\n",
    "# notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
